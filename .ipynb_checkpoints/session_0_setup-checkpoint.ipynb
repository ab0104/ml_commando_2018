{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M/L Commando Course, Cambridge 2018\n",
    "## Session 0: Environment setup and iris classification\n",
    "### Russell Moore\n",
    "#### Based on the work of Guillermo Moncecchi, Diego Garat, Raúl Garreta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This session introduces the student to the sklearn environment, numpy and pandas.  We introduce estimators and scalers. Uses the Iris flower dataset, introduced in 1936 by Sir Ronald Fisher, to perform binary and multi-class classification._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing numpy, scikit-learn, and pyplot, the Python libraries we will be using. Show the versions we will be using (in case you have problems running the notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import scikit-learn, numpy, scipy and pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import IPython\n",
    "import platform\n",
    "    \n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sklearn.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every method implemented on scikit-learn assumes that data comes in a dataset. Scikit-learn includes a few well-known datasets. The [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) includes information about 150 instances from three different Iris flower species, including sepal and petal length and width. The natural task to solve using this dataset is to learn to guess the Iris species knowing the sepal and petal measures. Let's import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(type(iris)) # rjm49 - what datatype is this then?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes 150 instances, with 4 attributes each. For each instance, we will also have a target class (in our case, the species). This class is a special attribute which we will aim to predict for new, previously unseen instances, given the remaining (known) attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "print(type(X_iris), X_iris.shape)\n",
    "print(type(y_iris), y_iris.shape)\n",
    "\n",
    "print (\"\\nshapes of arrays:\", X_iris.shape, y_iris.shape) #shape is an attribute of numpy arrays\n",
    "\n",
    "\n",
    "#rjm49 - let's check out what datatypes we're dealing with...\n",
    "print(\"X entries are of data type:\",X_iris.dtype) # <-- expecting continuous values here (floating point numbers)\n",
    "print(\"y entries are of data type:\",y_iris.dtype)# <-- expecting category index here (integer)\n",
    "#numpy.ndarray types are NumPy n-dimensional arrays\n",
    "\n",
    "print ('\\nFeature names:{0}'.format(iris.feature_names))\n",
    "print ('Target classes:{0}'.format(iris.target_names))\n",
    "\n",
    "print ('\\nFirst instance features:{0}'.format(X_iris[0]))\n",
    "print(X_iris[0:5,:]) # <-- here we print the first 5 rows of X\n",
    "print(y_iris[0:5]) # <-- here we print the first 5 items of y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's see how we can create a boolean array from a numpy array using a comparator.  This is useful as a \"mask\" for filtering rows or columns out of other numpy arrays, as we'll see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((y_iris == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display each instance in a 2d-scatter plot, using first sepal measures, and then petal measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('sepal')\n",
    "colormarkers = [ ['red','s'], ['greenyellow','o'], ['blue','x']]\n",
    "for i in range(len(colormarkers)):\n",
    "    px = X_iris[:, 0][y_iris == i] #<-- selects all rows (:), first col (0), then filters where y_iris == i\n",
    "    py = X_iris[:, 1][y_iris == i] #<-- selects all rows (:), first col (0), then filters where y_iris == i\n",
    "    plt.scatter(px, py, c=colormarkers[i][0], marker=colormarkers[i][1])\n",
    "\n",
    "plt.title('Iris Dataset: Sepal width vs sepal length')\n",
    "plt.legend(iris.target_names)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.figure('petal')\n",
    "\n",
    "for i in range(len(colormarkers)):\n",
    "    px = X_iris[:, 2][y_iris == i]\n",
    "    py = X_iris[:, 3][y_iris == i]\n",
    "    plt.scatter(px, py, c=colormarkers[i][0], marker=colormarkers[i][1])\n",
    "\n",
    "plt.title('Iris Dataset: petal width vs petal length')\n",
    "plt.legend(iris.target_names)\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1936 Sir Ronald Fisher introduced the Iris dataset to the statistics world, using it to develop a _linear discriminant model_. What he did was to build a linear combination of the attributes that separates a species from the rest, that is, find a straight line similar to the one we suggested in the previous section.\n",
    "\n",
    "**Task 1:** predict the species of an iris flower given the four sepal and length measures. For the moment, we will start using only two attributes, its sepal width and length. We will do this to ease visualization, but later we will use the four attributes, and see if performance improves. This is an instance of a **classification problem**, where we want to assign a label taken from a discrete set to an item according to its features.\n",
    "\n",
    "The typical classification process roughly involves the following steps: \n",
    "- select your attributes, \n",
    "- build a model based on available data, and \n",
    "- evaluate your model’s performance on previously unseen data. \n",
    "\n",
    "To do this, before building our model we should separate training and testing data. Training data will be used to build the model, and testing data will be used to evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be to separate the dataset into to separate sets, using 75% of the instances for training our classifier, and the remaining 25% for evaluating it (and, in this case, taking only two features, sepal width and length). We will also perform _feature scaling_: for each feature, calculate the average, subtract the mean value from the feature value, and divide the result by their standard deviation. After scaling, each feature will have a zero average, with a standard deviation of one. This standardization of values (which does not change their distribution, as you could verify by plotting the X values before and after scaling) is a common requirement of machine learning methods, to avoid that features with large values may weight too much on the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create dataset with only the first two attributes\n",
    "X, y = X_iris[:, [0,1]], y_iris\n",
    "# Test set will be the 25% taken randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    "    \n",
    "# Standarize the features\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that, after scaling, the mean is 0 and the standard deviation is 1 (this should be exact in the training set, but only approximated in the testing set, because we used the training set media and standard deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Training set mean:{:.2f} and standard deviation:{:.2f}'.format(np.average(X_train),np.std(X_train)))\n",
    "print ('Testing set mean:{:.2f} and standard deviation:{:.2f}'.format(np.average(X_test),np.std(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the training data, after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormarkers = [ ['red','s'], ['greenyellow','o'], ['blue','x']]\n",
    "plt.figure('Training Data')\n",
    "for i in range(len(colormarkers)):\n",
    "    xs = X_train[:, 0][y_train == i]\n",
    "    ys = X_train[:, 1][y_train == i]\n",
    "    plt.scatter(xs, ys, c=colormarkers[i][0], marker=colormarkers[i][1])\n",
    "\n",
    "plt.title('Training instances, after scaling')\n",
    "plt.legend(iris.target_names)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A linear, binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's transform the problem to a binary classification task: we will only want to distinguish setosa flowers from the rest (it seems easy, according to the plot). To do this, we will just collapse all non-setosa targets into the same class (later we will come back to the three-class original problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "y_train_setosa = copy.copy(y_train) # <-- python newbies need to understand copy.copy()!\n",
    "# Every instance of 1 and 2 in the training set will set to 1\n",
    "y_train_setosa[y_train_setosa > 0]=1\n",
    "y_test_setosa = copy.copy(y_test)\n",
    "y_test_setosa[y_test_setosa > 0]=1\n",
    "\n",
    "print ('Old training target classes:\\n{0}'.format(y_train))\n",
    "\n",
    "print ('New training target classes:\\n{0}'.format(y_train_setosa))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first classifier will be a linear one. \n",
    "\n",
    "Linear classification models have been very well studied through many years, and the are a lot of different methods with actually very different approaches for building the separating hyperplane.  We will use the `LogisticRegression` classifier, which is simple and quick.  The technique was invented by Cambridge Mathematician David Cox in 1958.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every classifier in scikit-learn is created the same way: calling a method with the classifier's configurable hyperparameters to create an instance of the classifier. In this case, we just call the constructor with the default (empty brackets) parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model \n",
    "#clf = linear_model.SGDClassifier(loss='log', random_state=42, fit_intercept=True)\n",
    "clf = linear_model.LogisticRegression()\n",
    "print (clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the classifier includes several parameters. Usually, scikit-learn specifies default values for every parameter. But be aware that it is not a good idea to keep it with their default values.  Normally we would undergo a process of _parameter tuning_ to find the optimal parameters.\n",
    "\n",
    "Now, we just call the `fit` method to train the classifier (i.e., build a model we will later use), based on the available training data. In our case, the trainig setosa set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train_setosa)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our model look? Well, since we are building a linear classifier, our model is a... line. We can show its coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (clf.coef_,clf.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we can draw the decision boundary using pyplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
    "y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
    "xs = np.arange(x_min, x_max, 0.5)\n",
    "fig,axes = plt.subplots()\n",
    "axes.set_aspect('equal')\n",
    "axes.set_title('Setosa classification')\n",
    "axes.set_xlabel('Sepal length')\n",
    "axes.set_ylabel('Sepal width')\n",
    "axes.set_xlim(x_min, x_max)\n",
    "axes.set_ylim(y_min, y_max)\n",
    "plt.sca(axes)\n",
    "plt.scatter(X_train[:, 0][y_train == 0], X_train[:, 1][y_train == 0], c='red', marker='s')\n",
    "plt.scatter(X_train[:, 0][y_train == 1], X_train[:, 1][y_train == 1], c='black', marker='x')\n",
    "ys = (-clf.intercept_[0]- xs * clf.coef_[0, 0]) / clf.coef_[0, 1]\n",
    "plt.plot(xs, ys) #, hold=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line is our decision boundary. Whenever $((30.97 \\times sepal\\_length) - (17.82 \\times sepal\\_width) - 17.34) \\gt 0$, we're looking at an iris setosa (class 0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the really useful part: when we have a new flower, we just have to get its petal width and length and call the `predict` method of the classifier on the new instance. _This works the same way no matter the classifier we are using or the method we used to build it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rjm49 - I've separated out these steps for clearer reading\n",
    "new_flower = [[4.7, 3.1]]\n",
    "scaled = scaler.transform(new_flower)\n",
    "print(\"scaled this new instance to match the range of the others:\", scaled)\n",
    "prediction = clf.predict(scaled)\n",
    "print ('If the flower has 4.7 petal width and 3.1 petal length, predicted class is {}, so a {}'.format(\n",
    "        prediction, iris.target_names[prediction]))\n",
    "\n",
    "predict_prob = clf.predict_proba(scaled)\n",
    "print(\"Class probabilities are {}\".format(predict_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we first scaled the new instance, then applied the `predict` method, and used the result to lookup into the iris target names arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "print (metrics.classification_report(y_test_setosa, y_pred, target_names=['setosa','other']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the original three-class problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the training using the three original classes. Using scikit-learn this is simple: we do exactly the same procedure, using the original three target classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf2 = linear_model.SGDClassifier(loss='log', random_state=33)\n",
    "#clf2 = svm.LinearSVC()\n",
    "clf2 = linear_model.LogisticRegression(solver=\"newton-cg\")\n",
    "clf2.fit(X_train, y_train) \n",
    "print (\"number of classes is:\", len(clf2.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have _three_ decision curves... scikit-learn has simply converted the problem into three one-versus-all binary classifiers. Note that Class 0 is linearly separable, while Class 1 and Class 2 are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
    "y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
    "xs = np.arange(x_min,x_max,0.5)\n",
    "fig, axes = plt.subplots(1,3)\n",
    "fig.set_size_inches(10,6)\n",
    "for i in [0,1,2]: # <-- for each class of flower...\n",
    "    flower_name = iris.target_names[i]\n",
    "    axes[i].set_aspect('equal')\n",
    "    axes[i].set_title('Class '+ flower_name + ' versus the rest')\n",
    "    axes[i].set_xlabel('Sepal length')\n",
    "    axes[i].set_ylabel('Sepal width')\n",
    "    axes[i].set_xlim(x_min, x_max)\n",
    "    axes[i].set_ylim(y_min, y_max)\n",
    "    plt.sca(axes[i])\n",
    "    ys=(-clf2.intercept_[i]-xs*clf2.coef_[i,0])/clf2.coef_[i,1]\n",
    "    # add a plot using xs, ys to show decision boundary\n",
    "    for j in [0,1,2]:\n",
    "        px = X_train[:, 0][y_train == j]\n",
    "        py = X_train[:, 1][y_train == j]\n",
    "        color = colormarkers[j][0] if j==i else 'black'\n",
    "        marker = 'o' if j==i else 'x'\n",
    "        # add a scatter plot to show the sample points\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate on the previous instance to find the three-class prediction. Scikit-learn tries the three classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_flower = [[4.7, 3.1]] # try some other values here to see what you get\n",
    "flower = scaler.transform(raw_flower)\n",
    "print(\"setosa score\",clf2.decision_function(flower)[0][0])\n",
    "print(\"versicolor score\", clf2.decision_function(flower)[0][1])\n",
    "print(\"virginica score\", clf2.decision_function(flower)[0][2])\n",
    "\n",
    "pred = clf2.predict(flower)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `decision_function` method tell us the classifier scores (in our case, the left side of the decision boundary inequality). In our example, the first classifier says the flower is a setosa (we have a score greater than zero), and it is not a versicolor nor a virginica. Easy. What if we had two positive values? In our case, the greatest score will be the point which is further away from the decision line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of an estimator is a measure of its effectiveness. The most obvious performance measure is called _accuracy_: given a classifier and a set of instances, it simply measures the proportion of instances correctly classified by the classifier. We can, for example, use the instances in the training set and calculate the accuracy of our classifier when predicting their target classes. Scikit-learn includes a `metrics` module that implements this (and many others) performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_train_pred = clf2.predict(X_train)\n",
    "print ('Accuracy on the training set:{:.2f}'.format(metrics.accuracy_score(y_train, y_train_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our classifier correctly predicts 83% of the instances in the training set. But this is actually a bad idea.  The problem with the evaluating on the training set is that you have built your model using this data, and it is possible that your model adjusts actually very well to them, but performs poorly in previously unseen data (which is its ultimate purpose). This phenomenon is called overfitting, and you will see it once and again while you read this book. If you measure on your training data, you will never detect overfitting. So, _never ever_ measure on your training data. \n",
    "\n",
    "Remember we separated a portion of the training set? Now it is time to use it: since it was not used for training, we expect it to give us and idead of how well our classifier performs on previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf2.predict(X_test)\n",
    "print ('Accuracy on the test set:{:.2f}'.format(metrics.accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, accuracy on the testing set is lower than the accuracy on the training set, since the model is actually modeling the training set, not the testing set.\n",
    "\n",
    "One of the problems with accuracy is that does not reflect well how our model performs on each different target class. For example, we know that our classifier works very well identifying setosa species, but will probably fail when separating the other two species. If we could measure this, we could get hints for improving performance, changing the method or the features. \n",
    "\n",
    "A very useful tool when facing multi-class problems is the confusion matrix. This matrix includes, in element (r,c) the number of instances of class r that were predicted to be in class c. A good classifier will accumulate the values on the confusion matrix diagonal, where correctly classified instances belong. Having the original and predicted classes, we can easily print the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the classification matrix, just remember the definition: the “8” on row 2, column 3, means that eight instances if class 1 where predicted to be in class 2. Our classifier is never wrong in our evaluation set when it classifies class zero (setosa) flowers.  However, when it faces classes one and two (versicolor and virginica), it confuses them. The confusion matrix gives us useful information to know what kind of errors the classifier is making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the test set is a good performance measure when the number of instances of each class is similar, i.e., we have a uniform distribution of classes. However, consider that 99 percent of your instances belong to just one class (you have a skewed dataset): a classifier that always predicts this  majority class will have an excellent performance in terms of accuracy, despite the fact that it is an extremely naive method (and that it will surely fail in the “difficult” 1% cases).\n",
    "\n",
    "Within scikit-learn, there are several evaluation functions; we will show three popular ones: precision, recall, and F1-score (or f-measure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision: what proportion of instances predicted to be in some class C, are actually in C.\n",
    "- Recall: what proportion of all instances in class C were correctly predicted.\n",
    "- F1-score is the harmonic mean of precision and recall, and tries to combine both in a single number (namely $F_1 = \\frac{2pr}{p+r}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the four flower attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end with this classification section, we will repeat the whole process, this time using the four original attributes, and check if performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set will be the 25% taken randomly\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X_iris, y_iris, test_size=0.25, random_state=33) # try using a different train/test split\n",
    "    \n",
    "# Standarize the features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler = None # fit the scaler on the training set\n",
    "X_train4 = scaler.transform(X_train4)\n",
    "X_test4 = scaler.transform(X_test4)\n",
    "\n",
    "# Build the classifier\n",
    "clf3 = None # instantiate your own classifier here\n",
    "clf3.fit(X_train4, y_train4) \n",
    "\n",
    "# Evaluate the classifier on the evaluation set\n",
    "y_pred4 = None # insert code here to predict y values\n",
    "print (metrics.classification_report(y_test4, y_pred4, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end with this introductory notebook, we will summarise the main common steps to apply a supervised learning method in scikit-learn:\n",
    "-\tGet your dataset. Select your learning features, and create a 2D data array, with one column for each feature, and one row for each learning instance. Every feature is represented by a real number. Your data probably does not look like that in its original format. In the real world, this the preprocessing stage usually takes a lot of time. In the next chapters we will show several examples of preprocessing different types of data. \n",
    "-\tCreate an instance of an estimator (a classifier or a regressor). In scikit-learn, this corresponds to an object that implements the methods `fit(X,y)` and `predict(T)`. This estimator takes as arguments the model’s parameters. You can set these parameters by hand, or using tools for model selection (we will address this later in this book).\n",
    "-\tSeparate you training and testing sets (or, alternatively, use cross-validation)\n",
    "-\tBuild the model using the `fit(X,y)` method, being X your training data, and y the corresponding target class. \n",
    "-\tEvaluate you estimator on the testing data, using the `predict(T)` method, where T is your test dataset.\n",
    "-\tCompare your results with the original target classes, using one of the several measures in the metrics module.\n",
    "\n",
    "The previous steps are only an overview: scikit-learn offers several additional methods implementing different machine learning operations (such as dimensionality reduction, clustering, or semi-supervised learning), and many dataset transformation methods. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
